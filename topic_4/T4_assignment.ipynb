{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 - By Wilson Peguero Rosario\n",
    "\n",
    "## Problem Statement\n",
    "In this assignment, the learning rule for perceptron is examined as an example of proper network behavior. The notions of gradient descent and stochastic gradient descent are emphasized, as well as post-training and the implication of fitting a model to a training data set. Overfitting is adderessed and the tradeoff of bias as compared to variance is also discussed.\n",
    "\n",
    "Consider a signle neuron perceptron with a hard limit transfer function (hardlim).\n",
    "\n",
    "## Solution\n",
    "\n",
    "A perceptron can be considered to be an artificial neuron. It receives input from other perceptrons or from sensors and has a threshold at which the perceptron decides to send a signal down the chain or not. This can be considered to be the activation and in this case, there is no gradual transition from 0 to 1 as the hard limit function is 0 where x<0 and is 1 where x>=0.\n",
    "\n",
    "To better represent a perceptron, one can create a perceptron class with the activation function being the hard lim function.\n",
    "\n",
    "*Importing the Necessary Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the library have been imported, the next step is to create the perceptron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Simple Perceptron\n",
    "\n",
    "    This is a representation of a perceptron within an Artificial Neural Network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Number of times algorithm passes over the training data set.\n",
    "    random_state : int\n",
    "        Random number generator seed for random weight initialization.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    w : 1D-array\n",
    "        Weights after fitting.\n",
    "    \n",
    "    errors : list\n",
    "        Number of misclassifications, or updates, in each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.1, n=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n = n\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trains the model based on  the given data for making predictions\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        X : array-like, shape = [samples, features]\n",
    "            - Vectors used for training the model.\n",
    "            - length of the vectors is equivalent to the number of features.\n",
    "        \n",
    "        y : array-like, shape = [samples]\n",
    "            - Target values.\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w = rgen.normal(loc=0.0, scale=0.01, size=1+X.shape[1])\n",
    "        self.errors = []\n",
    "        for i in range(self.n):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X,y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w[1:] += update * xi\n",
    "                self.w[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors.append(errors)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate the net input.\n",
    "        \n",
    "        This is where the sum of the products between the weights and the inputs are calculated.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w[1:]) + self.w[0]\n",
    "    \n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Activation function.\"\"\"\n",
    "        return 1.0 if self.net_input(X) >= 1 else 0.0\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step.\"\"\"\n",
    "        return np.where(self.activation(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is an example of a perceptron with a hard limit function as its activation function. Now moving on to the decision boundaries, we draw the decision boundaries in red marker (as shown below)\n",
    "\n",
    "![decision_boundaries](T4_decision_boundary.png)\n",
    "\n",
    "To find the weights and bias of the single-neuron perceptron, one must first consider that the points where the single neuron perceptron's decision boundaries intercepts are where the function is equal to 0.\n",
    "\n",
    "Taking into account part a, the net input function is $f(x_1, x_2) = w_1 x_1 + w_2 x_2 + b =  0$\n",
    "\n",
    "When $x_1 = 0$ and $x_2 = 0$, then $f(x_1, x_2) = 0$. This then allows us to learn the value of the bias (which is 0) and in turn convert the equation to the following function:\n",
    "\n",
    "$f(x_1, x_2) = w_1 x_1 + w_2 x_2 = 0$\n",
    "\n",
    "Now the nature of the decision boundary is such that $x_2 = 2 x_1$\n",
    "\n",
    "This means that the weights can be considered to be the following:\n",
    "\n",
    "- $w_1 = -2$\n",
    "- $w_2 = 1$\n",
    "\n",
    "Taking into account part b, the net input function is $f(x_1, x_2) = w_1 x_1 + w_2 x_2 + b =  0$\n",
    "\n",
    "Based on the nature of the boundary shown on b, one can immediately tell that it is of the nature y=x where y is a constant (x = -1).\n",
    "\n",
    "One can then reformulate the equation for part b above and obtain the following formula:\n",
    "\n",
    "$f(x_1, x_2) = w_1 x_1 + w_2 (-1) + b =  0$\n",
    "\n",
    "$f(x_1, x_2) = w_1 x_1 - w_2 + b =  0$\n",
    "\n",
    "Now we substitute for the point where $x_1 = 0$ and obtain the following formula:\n",
    "\n",
    "$w_2 = b$\n",
    "\n",
    "substituting again, one learns that the weight $w_1 = 0$ and that the second weight is $w_2=1$ and the bias is 1. We obtain the formula below based on the aformentioned:\n",
    "\n",
    "$f(x_1, x_2) = 0 * x_1 - 1 + 1 =  0$\n",
    "\n",
    "Taking into account part c, the net input function is $f(x_1, x_2) = w_1 x_1 + w_2 x_2 + b =  0$\n",
    "\n",
    "There are two points where the decision boundary intercepts the parameters $x_1$ and $x_2$. These two points are (-3,0) and (0,3).\n",
    "\n",
    "Substituting for the first point, we obtain the following equation:\n",
    "\n",
    "$-3 w_1 + b = 0$\n",
    "\n",
    "$w_1 = b/3$\n",
    "\n",
    "$3 w_2 +b = 0$\n",
    "\n",
    "$w_2 = - b/3$\n",
    "\n",
    "Once can select any arbitrary bias which will provide one with the weights. For the sake of convenience, lets allow the bias to equal 3, this allows the weights to be the following:\n",
    "\n",
    "$w_1 = 1$\n",
    "\n",
    "$w_2 = -1$\n",
    "\n",
    "\n",
    "Now to find the variance one must first calculate the difference between predicted values and and the actual values of a data set. Once that difference has been discovered, one squares the sum of the differences and divides by the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x286f8243f40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_a = Perceptron()\n",
    "X = np.array(\n",
    "    [\n",
    "        [-2, -2],\n",
    "        [-2, 0],\n",
    "        [-2, 2],\n",
    "        [0, -2],\n",
    "        [0, 2],\n",
    "        [2, -2],\n",
    "        [2, 0],\n",
    "        [2, 2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_a = np.array(\n",
    "    [\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0],\n",
    "        [1],\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "    ]\n",
    ")\n",
    "perceptron_a.fit(X, y_a)\n",
    "\n",
    "perceptron_b = Perceptron()\n",
    "\n",
    "y_b = np.array(\n",
    "    [\n",
    "        [1],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1],\n",
    "        [0],\n",
    "        [1],\n",
    "        [0],\n",
    "        [0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "perceptron_b.fit(X, y_b)\n",
    "\n",
    "perceptron_c = Perceptron()\n",
    "\n",
    "y_c = np.array(\n",
    "    [\n",
    "        [1],\n",
    "        [1],\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "perceptron_c.fit(X, y_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the three models have been completed and trained, one should be able to make accurate predictions. One can compare with any new point or value as one also knows the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate for model a is 1\n",
      "The estimate for model b is 1\n",
      "The estimate for model c is 1\n"
     ]
    }
   ],
   "source": [
    "prd_a = perceptron_a.predict(np.array([1,1]))\n",
    "prd_b = perceptron_b.predict(np.array([1,1]))\n",
    "prd_c = perceptron_c.predict(np.array([-2,3]))\n",
    "\n",
    "print(f\"The estimate for model a is {prd_a}\")\n",
    "print(f\"The estimate for model b is {prd_b}\")\n",
    "print(f\"The estimate for model c is {prd_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the model is highly inaccurate, this is due to the fact that there are little to no data points to properly create the model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "381354fdf2db71db833037bd920d0e1d9b0c7ba1f03ade08c697116913672196"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
